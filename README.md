# AISmartMixer: AI 智能混剪工具【初版】

AISmartMixer 能够根据用户提供的旁白音频，智能地从视频素材库中挑选匹配的片段，并将它们组合成一个新的视频。该工具利用 AI 技术进行音频转录、语义理解和视频内容匹配，旨在简化视频内容创作流程。

## 主要功能

*   **音频分析**: 使用 OpenAI Whisper 模型将旁白音频转录成带时间戳的文本。
*   **视频内容理解**:
    *   扫描指定的视频素材文件夹（支持递归搜索）。
    *   从视频文件名中提取描述性文本。
    *   使用 SentenceTransformer 模型为视频描述生成语义嵌入向量。
    *   获取视频素材时长。
*   **智能匹配**: 基于音频片段的文本内容和视频片段的描述性文本之间的语义相似度（余弦相似度），为每个音频段落找到最佳的视频匹配。
*   **时间线生成**:
    *   根据音频和视频的匹配结果，生成一个编辑决策列表 (EDL)。
    *   支持配置视频片段的最小使用时长。
    *   包含视频复用策略，以在素材有限时填充时间线。
    *   若无匹配，可使用空白/黑场画面作为回退。
*   **视频合成**: 使用 MoviePy 库根据生成的 EDL 将选定的视频片段与原始旁白音频合成为最终视频。
*   **高度可配置**: 所有关键路径、模型选择、处理参数均可通过 `config.yaml` 文件进行配置。
*   **日志记录**: 详细的日志记录，方便追踪处理过程和调试。
*   **参数化执行**: 支持命令行参数，如强制重新分析、跳过输出文件夹清理。

## 技术栈

*   **Python 3.x**
*   **核心库**:
    *   `openai-whisper`: 音频转录
    *   `sentence-transformers`: 文本语义嵌入与相似度计算
    *   `moviepy`: 视频处理与合成
    *   `PyYAML`: 配置文件解析
    *   `numpy`: 数值计算

## 项目结构

    AISmartMixer/
    ├── src/                      # 主要源代码目录
    │   ├── main.py               # 主程序入口
    │   ├── phase_1_analyzer.py   # 音频和视频分析模块
    │   ├── phase_2_matcher.py    # 音频与视频匹配模块
    │   ├── phase_3_timeline_generator.py # 时间线生成模块
    │   ├── phase_4_synthesizer.py # 视频合成模块
    │   └── utils/                # 工具类模块
    │       ├── config_loader.py  # 配置文件加载
    │       ├── file_handler.py   # 文件读写处理
    │       └── logger_setup.py   # 日志配置
    ├── data/                     # 示例输入数据存放目录 (需用户自行创建和填充)
    │   ├── input_audio/          # 存放旁白音频文件
    │   └── input_video_clips/    # 存放B-roll视频素材片段
    ├── output/                   # 输出文件存放目录 (程序自动创建)
    │   ├── processed_data/       # 存放中间处理文件 (转录稿, 嵌入向量等)
    │   ├── edl/                  # 存放生成的编辑决策列表 (JSON格式)
    │   └── final_video/          # 存放最终合成的视频
    ├── config.yaml               # 项目配置文件
    └── requirements.txt          # Python依赖包列表


## 安装与环境准备

1.  **克隆代码库**:
    ```bash
    git clone <your-repository-url>
    cd AISmartMixer
    ```

2.  **安装 FFmpeg**:
    MoviePy 依赖 FFmpeg 进行视频处理。请确保您的系统已安装 FFmpeg 并将其添加至系统 PATH。
    *   **Windows**: 下载预编译版本，解压并将其 `bin` 目录添加到 PATH。
    *   **Linux (Ubuntu/Debian)**: `sudo apt update && sudo apt install ffmpeg`
    *   **macOS (Homebrew)**: `brew install ffmpeg`

3.  **创建 Python 虚拟环境** (推荐):
    ```bash
    python -m venv venv
    # Windows
    .\venv\Scripts\activate
    # Linux/macOS
    source venv/bin/activate
    ```

4.  **安装 Python 依赖**:
    ```bash
    pip install -r requirements.txt
    ```
    *注意: `openai-whisper` 可能需要 `rust` 和 `setuptools-rust`。如果安装 `openai-whisper` 时遇到编译问题，请先确保已安装 Rust 工具链。*

## 配置

核心配置位于项目根目录下的 `config.yaml` 文件。主要配置项包括：

*   **`paths`**:
    *   `input_audio_dir`: 输入旁白音频的文件夹路径。
    *   `input_audio_filename`: 旁白音频文件名。
    *   `input_video_dir`: 输入B-roll视频素材的文件夹路径。
    *   `output_processed_data_dir`: 中间处理文件的输出路径。
    *   `output_edl_dir`: EDL文件的输出路径。
    *   `output_final_video_dir`: 最终视频的输出路径。
    *   以及各输出文件的具体名称。
*   **`models`**:
    *   `whisper_model_name`: Whisper 模型名称 (如 `tiny`, `base`, `small`, `medium`, `large-v3`)。
    *   `embedding_model_name`: SentenceTransformer 模型名称 (如 `all-MiniLM-L6-v2`)，可以根据实际情况更换模型。
*   **`analysis`**:
    *   `recursive_video_search`: 是否递归搜索 `input_video_dir` 下的子文件夹。
*   **`settings`**:
    *   `video_extensions`: 支持的视频文件扩展名列表。
    *   `min_clip_duration_for_timeline`: 视频片段在最终时间线中的最小持续时间。
    *   `similarity_threshold`: 判断视频与音频相关的余弦相似度阈值。
    *   `top_k_candidates`: 为每个音频段考虑的候选视频数量。
    *   `video_reuse_strategy`: 视频复用策略。
    *   `fallback_strategy`: 无匹配视频时的回退策略。
    *   `output_video_resolution`: 输出视频分辨率 (`auto` 或 `WIDTHxHEIGHT`)。
    *   `output_video_fps`: 输出视频帧率。
    *   以及视频编解码器、FFmpeg线程数等合成参数。
*   **`logging`**:
    *   `level`: 日志级别 (如 `INFO`, `DEBUG`)。
    *   `log_file`: 日志文件路径。
*   **`force_reanalyze`**: 是否强制重新分析音频和视频，即使中间文件已存在。

**首次运行前，请务必根据您的实际文件存放情况修改 `config.yaml` 中的路径配置，并准备好输入音频和视频素材。**

## 如何运行

1.  **准备数据**:
    *   将您的旁白音频文件（如 `narration.mp3`）放入 `config.yaml` 中 `paths.input_audio_dir` 指定的文件夹，并确保 `paths.input_audio_filename` 与文件名一致。
    *   将您的B-roll视频素材片段放入 `config.yaml` 中 `paths.input_video_dir` 指定的文件夹。视频文件名应具有描述性，因为程序会尝试从中提取语义信息（例如 `海边日出-风景-高清.mp4`）。

2.  **修改配置**:
    根据您的需求和数据路径，仔细检查并修改 `config.yaml`。

3.  **运行主程序**:
    在项目根目录下，确保虚拟环境已激活，然后运行：
    ```bash
    python -m src.main
    ```
    或者，如果您想指定一个不同的配置文件：
    ```bash
    python -m src.main --config /path/to/your/custom_config.yaml
    ```

4.  **命令行参数**:
    *   `--config <path>`: 指定配置文件的路径 (默认为 `config.yaml`)。
    *   `--force-reanalyze`: 强制重新分析所有音频和视频，即使已存在之前的分析结果。
    *   `--skip-clear`: 跳过在处理前清空 `output/edl` 和 `output/processed_data` 文件夹的步骤。

5.  **查看输出**:
    *   中间文件（如转录稿、视频元数据和嵌入）会保存在 `output/processed_data/`。
    *   生成的编辑决策列表 (EDL JSON) 会保存在 `output/edl/`。
    *   最终合成的视频会保存在 `output/final_video/`。
    *   日志文件会保存在 `output/app.log` (或 `config.yaml` 中指定的路径)。

## 工作流程

1.  **初始化**: 加载配置，设置日志记录器。
2.  **(可选) 清理**: 清空旧的输出文件夹。
3.  **阶段 1: 分析 (Analysis)**
    *   **音频分析**: 使用 Whisper 模型转录旁白音频，生成带时间戳的文本段落。
    *   **视频分析**:
        *   扫描视频素材目录，找到所有符合条件的视频文件。
        *   从文件名提取描述性文本。
        *   使用 SentenceTransformer 模型为每个视频的描述文本生成语义嵌入向量。
        *   获取视频时长。
    *   保存转录稿和视频元数据（含嵌入向量）为 JSON 文件，供后续使用或跳过重复分析。
4.  **阶段 2: 匹配器初始化 (Matcher Initialization)**
    *   加载 SentenceTransformer 模型，用于计算文本间的语义相似度。
5.  **阶段 3: 时间线生成 (Timeline Generation)**
    *   遍历音频转录稿中的每个文本段落。
    *   使用匹配器计算该音频段落与所有可用视频片段描述之间的语义相似度。
    *   根据相似度得分、视频时长、复用策略等，为音频段落挑选一个或多个最合适的视频片段。
    *   生成一个详细的编辑决策列表 (EDL)，记录每个时间点应播放哪个视频的哪个部分。
    *   将 EDL 保存为 JSON 文件。
6.  **阶段 4: 合成 (Synthesizer)**
    *   读取 EDL 和原始旁白音频。
    *   使用 MoviePy 库，根据 EDL 的指示，从视频素材库中精确提取所需的视频片段。
    *   将这些视频片段按顺序拼接，调整大小和分辨率以匹配输出设置。
    *   将原始旁白音频附加到拼接好的视频上。
    *   渲染并输出最终的混剪视频文件。
7.  **完成**: 记录总处理时长。

---

## 后续可能的改进策略

### 提高输出多样性

1.  **多样化视频选择策略**:
    *   **N-Best 随机选择**: 不总是选择相似度最高的视频，而是在相似度最高的 N 个候选视频中引入一定的随机性进行选择，或者根据一个概率分布来选择。
    *   **避免短期重复**: 即使视频复用策略允许，也可以增加一个临时惩罚机制，降低刚使用过的视频（或主题非常相似的视频）在接下来几个片段中的选择优先级。
    *   **内容类别平衡**: 如果视频素材有类别标签（例如：人物、风景、动画、特写），可以尝试在时间线上平衡不同类别素材的出现，避免长时间单一类别。
    *   **视觉节奏控制**: 引入对视频片段平均镜头时长、动态程度（例如，通过分析帧间差异）的考量，以控制最终视频的视觉节奏。

2.  **增加转场效果**:
    *   在 `Synthesizer` 中，使用 MoviePy 的转场效果（如淡入淡出 `crossfadein`, `crossfadeout`，或更复杂的转场）来连接不同的视频片段，而不是简单的硬切。转场类型可以配置或根据内容动态选择。

3.  **动态调整片段时长**:
    *   当前似乎是让视频片段时长严格匹配音频段时长。可以考虑允许视频片段略长于或短于音频段（在一定容忍范围内），并通过慢放、快进或裁剪来适配，增加灵活性。

4.  **背景音乐/音效**:
    *   允许用户提供背景音乐库，根据旁白的情感或主题（如果能分析出来）选择合适的背景音乐，并自动调整音量。

5.  **文本叠加/字幕**:
    *   将音频转录的文本作为字幕叠加到视频上，提供字幕样式配置。

### 提高相似度匹配准确性与鲁棒性

1.  **更先进的嵌入模型**:
    *   **多模态模型**: 考虑使用像 CLIP 这样的多模态模型，它们能够直接理解图像/视频帧内容并将其与文本关联，而不仅仅依赖于文件名或手动描述。这需要提取视频的关键帧并进行分析。
    *   **领域特定微调**: 如果有特定领域的标注数据（文本描述-视频片段对），可以对 SentenceTransformer 或多模态模型进行微调，以提高在该领域的匹配性能。

2.  **改进视频内容描述的提取**:
    *   **元数据文件**: 允许用户提供一个额外的元数据文件（如 CSV 或 JSON），将视频文件名映射到更详细、更准确的文本描述、关键词标签等。
    *   **视频OCR/ASR**: 如果B-roll素材本身包含语音或文本信息，可以运行OCR提取画面中的文字，或ASR提取素材自带的音频内容，作为补充描述。
    *   **场景识别/对象检测**: 对视频关键帧进行场景识别或对象检测，将识别出的标签作为视频内容的补充描述。

3.  **优化匹配算法**:
    *   **加权相似度**: 结合多种信息源计算相似度，例如：文件名提取的文本相似度、用户提供描述的相似度、视觉内容分析的相似度（如果使用多模态模型），并为它们分配合适的权重。
    *   **上下文感知匹配**: 不仅考虑当前音频段与视频的匹配，还考虑前后音频段的语义连贯性，选择能够形成更流畅叙事的视频序列。
    *   **知识图谱增强**: 利用知识图谱来扩展查询。例如，如果音频提到“海滩”，知识图谱可以关联到“阳光”、“海鸥”、“沙滩伞”等相关概念，从而匹配到包含这些元素的视频，即使视频描述未直接提及“海滩”。

4.  **用户反馈机制**:
    *   （长期）如果这是一个持续使用的系统，可以设计一个界面让用户对匹配结果进行评价（好/一般/差），收集这些反馈来进一步优化匹配模型或调整阈值参数。

5.  **处理歧义和低信息量文本**:
    *   对于非常短或信息量低的音频片段（如“嗯”、“好的”），当前的相似度匹配可能效果不佳。可以设置一个最小信息量阈值，低于该阈值的音频片段可以采用不同的策略，如使用通用过渡画面、延续上一个画面的主题，或直接标记为需要人工干预。

这些策略的实现复杂度各不相同，可以根据项目需求和资源投入逐步迭代实现。
